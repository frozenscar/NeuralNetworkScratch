{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWt9kbVe3cc64rx+5jVHS7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frozenscar/NeuralNetworkScratch/blob/main/NeuralNetworkScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FPDFW1iISOND"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork():\n",
        "    def __init__(self,dims):\n",
        "      self.dims = dims\n",
        "      self.activations = []\n",
        "      self.init_weights()\n",
        "      self.dW = []\n",
        "\n",
        "    def init_weights(self):\n",
        "      self.W = []\n",
        "      for l in range(len(self.dims)-1):\n",
        "          self.W.append(torch.randn(self.dims[l]+1,self.dims[l+1]))\n",
        "\n",
        "    def printWeights(self):\n",
        "      print(\"Weights:\")\n",
        "      for i in range(len(self.W)):\n",
        "        print(self.W[i])\n",
        "\n",
        "    def printWeightsShapes(self):\n",
        "      print(\"Weights shapes:\")\n",
        "      for i in range(len(self.W)):\n",
        "        print(\"Weights \",i,self.W[i].shape)\n",
        "\n",
        "    def printActivations(self):\n",
        "      print(\"Activations:\")\n",
        "      for i in range(len(self.activations)):\n",
        "        print(\"Activation \",i,self.activations[i])\n",
        "\n",
        "\n",
        "    def printActivationsShapes(self):\n",
        "      print(\"Activations:\")\n",
        "      for i in range(len(self.activations)):\n",
        "        print(\"Activation \",i,self.activations[i].shape)\n",
        "\n",
        "\n",
        "    def printdWShapes(self):\n",
        "      print(\"dW:\")\n",
        "      for i in range(len(self.dW)):\n",
        "        print(\"dW \",i,self.dW[i].shape)\n",
        "\n",
        "    def add_bias(self,X):\n",
        "      return torch.cat((X,torch.ones(X.shape[0],1)),1)\n",
        "\n",
        "    def feedForward(self, X):\n",
        "        self.activations.clear()\n",
        "        for i in range(len(self.W)):\n",
        "            X = self.add_bias(X)\n",
        "            self.activations.append(X)\n",
        "\n",
        "            X = torch.mm( X, self.W[i])\n",
        "\n",
        "        return X\n",
        "\n",
        "    def lossFn(self,y,y_pred):\n",
        "      loss_gradient = 2*(y_pred-y)\n",
        "\n",
        "      return loss_gradient\n",
        "\n",
        "    def loss(self,y,y_pred):\n",
        "      return torch.mean((y_pred-y)**2)\n",
        "\n",
        "\n",
        "    def backProp(self,y_pred,y):\n",
        "      self.dW.clear()\n",
        "\n",
        "\n",
        "      dLdy = self.lossFn(y,y_pred)\n",
        "      dLdA = dLdy\n",
        "      for i in range(len(self.W)):\n",
        "        #IN the last layer We do not have the bias node\n",
        "        if i==0:\n",
        "          self.dW.insert(0,torch.mm(self.activations[-1-i].T,dLdA))\n",
        "\n",
        "        #We have bias nodes for all the remaining layers,\n",
        "        #Bias node is not associated with previous layers' weights\n",
        "        #Hence we remove the gradients associated with bias node.\n",
        "        else:\n",
        "          self.dW.insert(0,torch.mm(self.activations[-1-i].T,dLdA)[:,:-1])\n",
        "\n",
        "\n",
        "        if i==0:\n",
        "          dLdA = torch.mm(dLdA,self.W[len(self.W)-1-i].T)\n",
        "\n",
        "        #We will not use the error associated with bias node.\n",
        "        #because there are no other weights associated going backwards.\n",
        "        else:\n",
        "          dLdA = torch.mm(dLdA[:,:-1],self.W[len(self.W)-1-i].T)\n",
        "      return self.dW\n",
        "\n",
        "    def updateWeights(self,lr):\n",
        "      for i in range(len(self.W)):\n",
        "        self.W[i] = self.W[i] - lr*self.dW[i]\n",
        "\n",
        "    def train(self,X,y,lr,epochs):\n",
        "      l=0\n",
        "      for i in range(epochs):\n",
        "        if i%10==0:\n",
        "          print(\"Loss:\",l/10)\n",
        "          l=0\n",
        "        y_pred = self.feedForward(X)\n",
        "        l+=self.loss(y,y_pred)\n",
        "        self.backProp(y_pred,y)\n",
        "        self.updateWeights(lr)\n",
        "    def predict(self,X):\n",
        "      return self.feedForward(X)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "PNlNmqECSPKV"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork([2,5,1])\n",
        "#nn.printWeightsShapes()\n",
        "X=torch.tensor([[-1,3],[1,-1],[7,3],[9,-5],[-5,-3],[8,-3],[15,1],[33,-0.4],[-33,-1],[-13,4],[-1,8]])\n",
        "X = X/100\n",
        "y=torch.tensor([[1],[0],[1],[0],[0],[0],[1],[0],[0],[1],[1]])\n",
        "# nn.feedForward(X)\n",
        "# nn.printActivations()\n",
        "# nn.printActivationsShapes()\n",
        "\n",
        "# nn.backProp(X,y)\n",
        "# nn.printWeightsShapes()\n",
        "# nn.printdWShapes()\n",
        "nn.train(X,y,0.01,1000)\n",
        "y = nn.predict(torch.tensor([[-0.1,-0.1],[0.3,0.3]],dtype = torch.float32))\n",
        "\n",
        "print(y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7Irt4AfboDZ",
        "outputId": "a15e9f70-4fe4-45b8-85ef-f2547e33662c"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0\n",
            "Loss: tensor(0.3099)\n",
            "Loss: tensor(0.2036)\n",
            "Loss: tensor(0.1991)\n",
            "Loss: tensor(0.1948)\n",
            "Loss: tensor(0.1906)\n",
            "Loss: tensor(0.1864)\n",
            "Loss: tensor(0.1822)\n",
            "Loss: tensor(0.1780)\n",
            "Loss: tensor(0.1739)\n",
            "Loss: tensor(0.1697)\n",
            "Loss: tensor(0.1656)\n",
            "Loss: tensor(0.1614)\n",
            "Loss: tensor(0.1573)\n",
            "Loss: tensor(0.1532)\n",
            "Loss: tensor(0.1492)\n",
            "Loss: tensor(0.1452)\n",
            "Loss: tensor(0.1413)\n",
            "Loss: tensor(0.1374)\n",
            "Loss: tensor(0.1336)\n",
            "Loss: tensor(0.1300)\n",
            "Loss: tensor(0.1264)\n",
            "Loss: tensor(0.1229)\n",
            "Loss: tensor(0.1195)\n",
            "Loss: tensor(0.1163)\n",
            "Loss: tensor(0.1132)\n",
            "Loss: tensor(0.1102)\n",
            "Loss: tensor(0.1074)\n",
            "Loss: tensor(0.1047)\n",
            "Loss: tensor(0.1022)\n",
            "Loss: tensor(0.0997)\n",
            "Loss: tensor(0.0975)\n",
            "Loss: tensor(0.0953)\n",
            "Loss: tensor(0.0934)\n",
            "Loss: tensor(0.0915)\n",
            "Loss: tensor(0.0898)\n",
            "Loss: tensor(0.0882)\n",
            "Loss: tensor(0.0867)\n",
            "Loss: tensor(0.0854)\n",
            "Loss: tensor(0.1010)\n",
            "Loss: tensor(0.9394)\n",
            "Loss: tensor(0.1552)\n",
            "Loss: tensor(0.0952)\n",
            "Loss: tensor(0.0932)\n",
            "Loss: tensor(0.0913)\n",
            "Loss: tensor(0.0895)\n",
            "Loss: tensor(0.0879)\n",
            "Loss: tensor(0.0864)\n",
            "Loss: tensor(0.0851)\n",
            "Loss: tensor(0.0838)\n",
            "Loss: tensor(0.0826)\n",
            "Loss: tensor(0.0816)\n",
            "Loss: tensor(0.0806)\n",
            "Loss: tensor(0.0798)\n",
            "Loss: tensor(0.0790)\n",
            "Loss: tensor(0.0783)\n",
            "Loss: tensor(0.0776)\n",
            "Loss: tensor(0.0771)\n",
            "Loss: tensor(0.0765)\n",
            "Loss: tensor(0.0761)\n",
            "Loss: tensor(0.0757)\n",
            "Loss: tensor(0.0753)\n",
            "Loss: tensor(0.0750)\n",
            "Loss: tensor(0.0781)\n",
            "Loss: tensor(0.2702)\n",
            "Loss: tensor(0.5537)\n",
            "Loss: tensor(0.0856)\n",
            "Loss: tensor(0.0799)\n",
            "Loss: tensor(0.0791)\n",
            "Loss: tensor(0.0783)\n",
            "Loss: tensor(0.0777)\n",
            "Loss: tensor(0.0771)\n",
            "Loss: tensor(0.0765)\n",
            "Loss: tensor(0.0761)\n",
            "Loss: tensor(0.0756)\n",
            "Loss: tensor(0.0752)\n",
            "Loss: tensor(0.0749)\n",
            "Loss: tensor(0.0746)\n",
            "Loss: tensor(0.0743)\n",
            "Loss: tensor(0.0741)\n",
            "Loss: tensor(0.0739)\n",
            "Loss: tensor(0.0737)\n",
            "Loss: tensor(0.0736)\n",
            "Loss: tensor(0.0734)\n",
            "Loss: tensor(0.0733)\n",
            "Loss: tensor(0.0732)\n",
            "Loss: tensor(0.0731)\n",
            "Loss: tensor(0.0730)\n",
            "Loss: tensor(0.0729)\n",
            "Loss: tensor(0.0729)\n",
            "Loss: tensor(0.0728)\n",
            "Loss: tensor(0.0728)\n",
            "Loss: tensor(0.0727)\n",
            "Loss: tensor(0.0730)\n",
            "Loss: tensor(0.0781)\n",
            "Loss: tensor(0.1605)\n",
            "Loss: tensor(0.3706)\n",
            "Loss: tensor(0.1200)\n",
            "Loss: tensor(0.0760)\n",
            "Loss: tensor(0.0744)\n",
            "tensor([[-0.6880],\n",
            "        [ 3.6549]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TUDHpZl3bwhb"
      },
      "execution_count": 163,
      "outputs": []
    }
  ]
}